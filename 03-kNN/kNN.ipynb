{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ee0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import nan_euclidean_distances as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17067aae",
   "metadata": {},
   "source": [
    "A continuación veremos como opera kNN mediante un ejemplo muy sencillo pero ilustrativo y haremos los calculos para verificar los valores que imputa el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090ec2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2579c0",
   "metadata": {},
   "source": [
    "Ahora veremos que para el primer punto los 2 vecinos más cercanos son $[3, 4, 3], [np.nan, 6, 5]$ por lo tanto para usa el promedio de ambos en la tercera coordenada para imputar el nan, es decir $(3+5)/2=4$ por lo tanto $[1, 2, np.nan]->[1, 2, 4]$. Pero ¿como reproducimos los calculos anteriores?. A continuación lo veremos, primero calculando la matriz de distancia euclideana y haciendo los cálculos de cada una de los componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71f20684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  3.46410162,  6.92820323, 11.29158979],\n",
       "       [ 3.46410162,  0.        ,  3.46410162,  7.54983444],\n",
       "       [ 6.92820323,  3.46410162,  0.        ,  3.46410162],\n",
       "       [11.29158979,  7.54983444,  3.46410162,  0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist(X,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3d369",
   "metadata": {},
   "source": [
    "Recordemos que aquí la distancia con valores faltantes se define como:\n",
    "$$\n",
    "d_{x_0,x_1}=\\sqrt{\\left(weight* dist({x_0,x_1}\\right)}\n",
    "$$\n",
    "\n",
    "donde weight es el la dimensión entre el número de coordenadas no missing en los puntos, ejemplo en el punto $x_0=[1, 2, np.nan]$, $w=3/2$ entonces la distancia entre $x_0=[1, 2, np.nan]$ y $x_0=( [3, 4, 3]$ es: \n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{x_0,x_1}&=\\sqrt{\\left(3/2\\times dist({x_0,x_1}\\right)}\\\\\n",
    "&=\\sqrt{\\left(3/2\\times [(1-3)^2+(2-4)^2]\\right)} \\\\\n",
    "&=3.46410162\n",
    "\\end{align*}\n",
    "$$\n",
    "Ahora sí quisieramos calcular la distancia entre el punto $x_0$ y el punto $x_2=[np.nan, 6, 5]$ tendríamos que $w_{02}=3/1$ aquí el número de coordenadas presentes es 1 porque la primera coordenada de $x_0$ es nan y la segunda de $x_2$ también es nan, así que solamente nos queda una dimensión para operar\n",
    "$$\n",
    "\\begin{align*}\n",
    "d_{x_0,x_3}&=\\sqrt{\\left(3/1\\times dist({x_0,x_1}\\right)}\\\\\n",
    "&=\\sqrt{\\left(3\\times [(2-6)^2]\\right)} \\\\\n",
    "&=6.92820323\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edeee1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.33333333],\n",
       "       [4.        ],\n",
       "       [5.        ],\n",
       "       [7.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = [[np.nan], [4], [5], [7]]\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputer.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edb39b",
   "metadata": {},
   "source": [
    "Cuando tienes un punto en una sola dimensión dado que no puedes determinar cual es el vecino más cercano si usas Knn será equivalente a promediar sobre todos los puntos en el dataset, así knn es útil para n>=2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
